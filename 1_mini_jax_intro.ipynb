{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c1529f-b8ca-43d0-a4d2-699fcea4768c",
   "metadata": {},
   "source": [
    "# mini JAX intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e03678-63f6-4fcd-98e9-1c007080deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line below to install jax\n",
    "#!pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda73fa3-59b7-47a4-8ba4-804283e17ce2",
   "metadata": {},
   "source": [
    "## 1. Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2baa40-9bac-4c5f-b358-d55202129ab0",
   "metadata": {},
   "source": [
    "### Demo with an algebraic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e8555-f3a3-4953-aa14-609f716e8022",
   "metadata": {},
   "source": [
    "Let us see how to calculate the partial derivatives of any function with jax. Consider the function:\n",
    "$$\n",
    "f=\\frac{ \\tanh ^{-1}(x)}{1-e^{-x}} e^{-x}\n",
    "$$\n",
    "\n",
    "its derivative is:\n",
    "$$\n",
    "f'(x) = \\frac{\\left[\\tanh^{-1}(x)\\left(1-x^{2}\\right)-1\\right] e^{x} +1}{\\left(e^{x}-1\\right)^{2}\\left(x^{2}-1\\right)}\n",
    "$$\n",
    "\n",
    "Let us implement both and compare with the autodiff result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ac672-1b55-40b2-9f81-de1a165e0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the numpy API from jax, and the gradient function.\n",
    "from jax import numpy as jnp, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f4d28-e0be-4edb-bed3-a60583d82c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function above\n",
    "def f(x):\n",
    "    return jnp.arctanh(x) * jnp.exp(-x) / (1 - jnp.exp(-x))\n",
    "\n",
    "# its analytical derivative!\n",
    "def d_f(x):\n",
    "    num = (jnp.arctanh(x) * (1 - x**2) - 1) * jnp.exp(x) + 1\n",
    "    den = (jnp.exp(x) - 1)**2 * (x**2 - 1)\n",
    "    return num / den\n",
    "\n",
    "# its automatic derivative.\n",
    "d_f_auto = grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051afd3-d3a7-4bad-a61a-ed49ba2778bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing derivatives!\n",
    "x = -0.3\n",
    "print(f\"Derivative at x = {x:.03f}:\")\n",
    "print(f\"Symbolic:  {d_f(x):.05f}\")\n",
    "print(f\"Automatic: {d_f_auto(x):.05f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfac2f-9134-4fea-84da-900c4242a3f6",
   "metadata": {},
   "source": [
    "### More parameters!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52c5ad-a7bc-4811-a52d-e39c5bafe305",
   "metadata": {},
   "source": [
    "The above is an example of differentiation of a single parameter and single valued function. JAX can compute all kinds of jacobians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c962d-d2ba-41ca-a0e7-9f88c3865caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function of multiple inputs\n",
    "def polynomial(x):\n",
    "    return x[0]**6 + x[1]**5 + x[2]**4\n",
    "\n",
    "\n",
    "grad(polynomial)(jnp.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488241e-3a92-47b4-a577-b7be0c0f1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a curve\n",
    "def curve(x):\n",
    "    return jnp.array([x**2, x/2])\n",
    "\n",
    "from jax import jacobian\n",
    "\n",
    "# this is a vector tangent to your curve\n",
    "jacobian(curve)(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2b565-d661-42f2-842d-16b1cd18778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex even!!\n",
    "def some_holomorphic_func(z):\n",
    "    return 1. / (z-1.0-1j)\n",
    "\n",
    "jac = grad(some_holomorphic_func, holomorphic=True)\n",
    "\n",
    "jac(10.+0.5j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a2fc0-f7f9-4f9d-a546-fd8a21028fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your function is not holomorphic, its derivative cannot be represented by a single complex number.\n",
    "# you might instead be interested in other jax functions for directional-like partial derivatives, such as\n",
    "# jax.jvp (jacobian vector product), jax.vjp (vector jacobian product), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754be127-3f4e-47da-bba7-7a9ffaeb6b8d",
   "metadata": {},
   "source": [
    "### FIY: JAX handles nested arguments too ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08bd16-94ba-4bf7-97f4-bff6485cc259",
   "metadata": {},
   "source": [
    "You don't need to pack everything into arrays, JAX's internals can deal with python-like nested structures such as dictionaries, tuples ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8b0a3-6bf9-43a2-a386-3e10ea37a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_of_named_params(dictionary):\n",
    "    return dictionary['apples']**3 + dictionary['oranges']**2\n",
    "params = {'apples': 1.0, 'oranges': 1.0}\n",
    "\n",
    "jacobian(function_of_named_params)(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b22b2-710f-4228-8aaa-8e88ba3b42e8",
   "metadata": {},
   "source": [
    "### Going further with automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a4f7a-f55e-478c-b2b3-eae01a12da0d",
   "metadata": {},
   "source": [
    "These were the basics. When dealing with real problems, you might have to learn about\n",
    "- The difference between forward and reverse autodifferentiation (the latter is used in the `grad` function because it is more efficient for funtions that have more inputs than outputs)\n",
    "- Efficiently computing Jacobian-vector products and Hessian-vector products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e350e-9d59-435c-8d95-425ffde515f4",
   "metadata": {},
   "source": [
    "## 2. Just-in-time compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2e59e-5f87-49ba-a37f-bda4a7f064f9",
   "metadata": {},
   "source": [
    "The philosophy of JAX is writing simple, atomic functions and composing them, then just-in-time compile the top function for optimal performance. Let us see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0ff92-04e6-47a2-b6db-56e4e38c963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the just in time compiler \n",
    "from jax import jit\n",
    "from time import time\n",
    "\n",
    "# a function doing some processing:\n",
    "def some_function():\n",
    "    x = 2*jnp.ones(10)\n",
    "    for _ in range(50):\n",
    "        x = x**0.5\n",
    "    return x\n",
    "\n",
    "# first run it without compiling:\n",
    "t0 = time()\n",
    "some_function()\n",
    "print(f\"Execution time without jit: {1000*(time()-t0):.02f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f534eed-caef-469f-8650-ba1f72f09e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's jit it.\n",
    "jitted = jit(some_function)\n",
    "# run it a first time ...\n",
    "t0 = time()\n",
    "jitted()\n",
    "print(f\"Execution time including compiling: {1000*(time()-t0):.02f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ebf74-1602-4b1b-afde-6391008ece98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and again\n",
    "t0 = time()\n",
    "jitted()\n",
    "print(f\"Execution time after compiling: {1000*(time()-t0):.02f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc622eac-f34a-4a27-8d29-a2a7ad54bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the construct you will see most of the time is the use of jit as a decorator:\n",
    "@jit\n",
    "def my_function():\n",
    "    pass\n",
    "# is strictly equivalent to\n",
    "def my_function():\n",
    "    pass\n",
    "my_function = jit(my_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15beb4af-538a-42a3-b04f-97bf97faa389",
   "metadata": {},
   "source": [
    "**What makes jit so fast?**\n",
    "- The function is converted to [XLA](https://openxla.org/xla#:~:text=XLA%20(Accelerated%20Linear%20Algebra)%20is,%2C%20CPUs%2C%20and%20ML%20accelerators) (accelerated linear algebra), then the XLA compiler does most of the magic.\n",
    "- In the present case, it might unroll the `for` loop: it identifies that the loop just does repeated operations without external dependencies.\n",
    "- In the present case still, it might even identify what we should have noticed when writing the code: repeated exponentiation is just exponentiating once with the sum of the exponents.\n",
    "- **And the best part:** XLA understands your hardware, whether it is a CPU, GPU, or other accelerator, it knows exactly how to cache things in an optimal way, eliminating overheads at every level!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b4406-7748-4def-8f41-28ffe657cc12",
   "metadata": {},
   "source": [
    "## 3. Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828973a-97ec-456a-851e-68c232b65756",
   "metadata": {},
   "source": [
    "We wrote a for loop in the previous example. Do not ever do this again when writing in JAX. In particular, looping over arrays is extremely unefficient, yet the compiler might still try and unroll such loops, resulting in compile times possibly longer than the age of the Universe. \n",
    "\n",
    "Instead, use batching, which the JAX API makes incredibly easy to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5efb4-4293-423a-8141-6a78202be4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # to generate some fake data\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699d213-dbc0-444b-980b-a55a9dcb077b",
   "metadata": {},
   "source": [
    "Say we are filtering some input data, each of these data an array of size 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a270db3-7095-4d02-a0ff-515fdc5309c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some 'data' ...\n",
    "y = np.random.normal(size=1000)\n",
    "\n",
    "# we'll smooth it with a moving average.\n",
    "def moving_average(data, window_size=60):\n",
    "    cumsum = jnp.cumsum(data)\n",
    "    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y, '.')\n",
    "plt.plot(moving_average(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d7004-0ae0-4e63-8f81-709ce334cde3",
   "metadata": {},
   "source": [
    "Now, say we have a bunch of data coming in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff9314-7aa6-4fbc-a921-e9c5f83fef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 times the 'data' of the above plot\n",
    "ys = jnp.array([np.random.uniform(size=1000) for _ in range(20)])\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214c773-9212-400b-9690-0440967d22eb",
   "metadata": {},
   "source": [
    "Start with the wrong way of doing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5285d3-4ded-4045-8699-0d9ccec93826",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def fit_all_data_loop(ys):\n",
    "    smoothed = []\n",
    "    for y in ys:\n",
    "        smoothed.append(moving_average(y))\n",
    "    return smoothed\n",
    "\n",
    "t0 = time()\n",
    "fit_all_data_loop(ys)\n",
    "print(f\"Execution time including compiling: {1000*(time()-t0):.02f} ms\")\n",
    "\n",
    "t0 = time()\n",
    "fit_all_data_loop(ys)\n",
    "print(f\"Execution time after compiling: {1000*(time()-t0):.02f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d6283-1476-4ba2-acf7-ce322c26deab",
   "metadata": {},
   "source": [
    "More than a second of compile time! Ages for a computer. It can get a lot worse, try increasing the number of observations to 1000 if you feel like having to kill the notebook completely to recover.\n",
    "\n",
    "And now the right way, we will batch over the first axis of our observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65941b87-e2e4-438c-ade8-69fee7a7bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap  # vectorized map! how to do complex operations on a vector of length N in O(1) time.\n",
    "@jit\n",
    "def fit_all_data(ys):\n",
    "    batched = vmap(moving_average, in_axes=(0,))\n",
    "    return batched(ys)\n",
    "\n",
    "t0 = time()\n",
    "fit_all_data(ys)\n",
    "print(f\"Execution time including compiling: {1000*(time()-t0):.02f} ms\")\n",
    "\n",
    "t0 = time()\n",
    "fit_all_data(ys)\n",
    "print(f\"Execution time after compiling: {1000*(time()-t0):.02f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc7128-0768-4a39-8d5c-6f5d96fa8e89",
   "metadata": {},
   "source": [
    "## 4. Putting it together in a toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db98b9-383b-40a4-9004-617a77fc18f9",
   "metadata": {},
   "source": [
    "The most widespread usage of automatic differentiation are optimization problems. Let us imagine a very stupid scenario, where we want to recover the \"true signal\" $t$ hidden in a noisy / blurred observation $o$.\n",
    "\n",
    "$$\n",
    "observation = kernel * truth + noise\n",
    "$$\n",
    "\n",
    "This is the classic ill-posed problem of deconvolution. JAX doesn't magically solve it, but it does give us more direct ways to tackle it.\n",
    "\n",
    "We start by generating some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfa96-40ae-44ba-a892-e076d58d4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our true signal\n",
    "truth = np.load('true_signal.npy')\n",
    "# and the \"instrument response\", just something to blur the true signal with.\n",
    "kernel = np.load('kernel.npy')\n",
    "# our observation, blur with the kernel and add noise.\n",
    "np.random.seed(0)\n",
    "obs = np.convolve(truth, kernel, mode='same') + np.random.normal(scale=0.15, size=truth.size)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(obs, '.', label='observed signal', mfc='None', color='gray')\n",
    "#plt.plot(truth, color='mediumblue', ls='--', label='true signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435f3e6-93af-452e-9680-a74bf2263da0",
   "metadata": {},
   "source": [
    "Our observations are blurry and noisy: potentially hiding the nature of the true signal. Luckily, in this example we know exactly what the instrumental blurring is! We can therefore forward model the data points with **very** minimal assumptions. While traditional methods would require modelling with very coercive functional forms, `JAX` allows us to optimize a single parameter per data point, and add a condition of smoothness to our model. This condition of smoothness is the very minimal assumption we are making.\n",
    "\n",
    "#### The Plan\n",
    "- Define a model, that is a function, `compute_model`, that transforms our *estimated* true signal to what we expect to observe.\n",
    "- Define a way to enforce smoothness, we'll request the total variation of the signal to be decently small: function `regularization`.\n",
    "- Define a data fidelity term, requiring that our model is similar to the input data, in `chi2`.\n",
    "- Combine all this in the `cost` function, of which we'll immediately calculate the gradient! The gradient will guide any optimizer that can use it to a local minimum, very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d870f8b-9fe9-4d94-b9ac-32f2690e018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple model ...our estimated true model is blurred by the response to yield the observed signal.\n",
    "def compute_model(estimated_truth):\n",
    "    # Here the model is very simple, just bluring our estimated signal with the instrumental response. \n",
    "    # The point, however, is that this can become arbitrarily complex, modelling physics, instrumental effects, anything. \n",
    "    model = jnp.convolve(estimated_truth, kernel, mode='same')\n",
    "    return model\n",
    "\n",
    "# an easy prior on what the reconstruction should look like is something smooth. \n",
    "def regularization(estimated_truth):\n",
    "    # Again, this can also be a lot more complex. We could even encode priors on what our solution should look like in neural networks.\n",
    "    # JAX would calculate the gradient across a huge neural network all the same.\n",
    "    return jnp.sum(jnp.abs(jnp.diff(estimated_truth)))\n",
    "\n",
    "# the data fidelity term ...our reconstructed model should match the observation\n",
    "def chi2(estimated_truth, obs):\n",
    "    model = compute_model(estimated_truth)\n",
    "    return jnp.sum( (model - obs)**2 )  # assuming uniform, unit variance for this example.\n",
    "\n",
    "# and the metric we will minimize, a mixture of recreating the observed data while being smooth enough.\n",
    "def cost(estimated_truth, obs):\n",
    "    return chi2(estimated_truth, obs) + 0.05*regularization(estimated_truth)\n",
    "\n",
    "# get the partial derivatives of this relatively long sequence of operations for free!\n",
    "cost_grad = jit(grad(cost, argnums=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06881935-5dc4-4011-a431-72b5f4d1bec5",
   "metadata": {},
   "source": [
    "Now we can just come up with an initial guess of what the true signal might look like ...or just start from zeros everywhere. Then we optimize them as to minimize our cost function with any minimization algorithm that can take advantage of the gradient.\n",
    "\n",
    "(Try one that doesn't by replacing the minimize statement with this!)\n",
    "```\n",
    "res = minimize(cost, method='Nelder-Mead', x0=params_ini, args=(obs,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2649b-dee1-4982-8cb9-12aed7cbb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "estimated_truth_ini = jnp.zeros_like(obs)\n",
    "# just use some algorithm that uses the gradient to find optimal parameters.\n",
    "res = minimize(cost, jac=cost_grad, method='CG', x0=estimated_truth_ini, args=(obs,))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(obs, '.', label='observed signal', mfc='None', color='gray')\n",
    "plt.plot(truth, color='mediumblue', ls='--', label='true signal')\n",
    "plt.plot(res.x, label='recovered signal', color='darkorange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d999cf9-9b01-4e6b-be83-c812553f8c17",
   "metadata": {},
   "source": [
    "We recover some information! At least, we get a hint that our true data might be multimodal. How can we be sure? What about throwing more data at it? After all, astronomical data rarely come alone. So, how do we combine the information of multiple observations?\n",
    "\n",
    "First let's make some independent observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515eb2d-4dcb-4160-bbcc-2f416037bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some independent observations\n",
    "obs = jnp.array([np.convolve(truth, kernel, mode='same') + np.random.normal(scale=0.15, size=truth.size) for _ in range(5)])\n",
    "\n",
    "# what do we get from averaging?\n",
    "plt.figure()\n",
    "plt.plot(np.mean(obs, axis=0), '.')\n",
    "plt.title('Average of multiple observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c2f86-ea9a-49aa-b4f6-24217c32c833",
   "metadata": {},
   "source": [
    "Averaging doesn't exactly help.\n",
    "\n",
    "Let's instead forward model with `JAX` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ac71e-c6cd-4df9-8e4a-51f4a5758c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch the cost function so it handles multiple inputs at once\n",
    "chi2_vmap = jit(vmap(chi2, in_axes=(None, 0)))\n",
    "# in_axes=(None, 0) means: do not batch on the first argument, and batch on the first axis of the second argument.\n",
    "\n",
    "# we also need to modify our cost function. Just taking the mean of the data fidelity term here.\n",
    "def cost_batched(params, obs):\n",
    "    return jnp.sum(chi2_vmap(params, obs)) + 0.05 * regularization(params)\n",
    "\n",
    "\n",
    "# gradient of an even longer list of operations now! still free\n",
    "cost_batched_grad = jit(grad(cost_batched, argnums=0))\n",
    "\n",
    "# fit our model to all this data at once.\n",
    "res = minimize(cost_batched, jac=cost_batched_grad, method='CG', x0=estimated_truth_ini, args=(obs,))\n",
    "\n",
    "plt.figure()\n",
    "for ob in obs:\n",
    "    plt.plot(ob, '.', mfc='None', color='gray')\n",
    "\n",
    "plt.plot(res.x, label='recovered signal', color='darkorange')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62a947-1b33-43fc-aed4-36ecf5a85317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
